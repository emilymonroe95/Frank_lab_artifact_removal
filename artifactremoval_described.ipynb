{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1662ad-9bb8-41e7-afdb-2c8d8ca1dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the process of defining a table. \n",
    "#A schema is a group of related tables\n",
    "#class corresponsing to table with given name, where it is inherting data from \n",
    "#Define the table\n",
    "#Anything above dashes is primary key- what uniquely defines the entry/'entity'\n",
    "#Anything below is a secondary key, parameters do not depend on parameter_dict, does depend on the name \n",
    "\n",
    "#If you call SpikeSortingArtifactDetectionParameters(), \n",
    "#I am confused about the functions. Actually called methods I believe? \n",
    "\n",
    "#Method called Insert_default. Is this \"run\"? A method is called to execute on that object. The object being the class/table. \n",
    "\n",
    "#So if you just call this table/class,\n",
    "#The first thing that will happen is that this code will insert an entry into the table with the default/none parameters from #insert default\n",
    "\n",
    "#Then, theres \"get no artifact times\"\n",
    "#Im confused, because it looks like this does the actual filtering somehow. Returns valid interval times, but its just the parameter table. \n",
    "#Maybe nothing is actually implemented here, but is defined to be called later on in the SpikeSortingRecording table \n",
    "#has something to do with the make method later probably \n",
    "\n",
    "@schema #define a schema, which is/ will be a group of related tables \n",
    "class SpikeSortingArtifactDetectionParameters(dj.Manual): #table creation, manually adding information to it. This table will hold the parameters for artifact detection. \n",
    "    #each set of parameters will have a name and a dictionary that contains the parameters, which will be used in the below function.  \n",
    "    definition = \"\"\"\n",
    "    # Table for holding parameters related to artifact detection\n",
    "    # Note that\n",
    "    artifact_parameter_name: varchar(200) #name for this set of parameters \n",
    "    ---\n",
    "    parameter_dict: BLOB    # dictionary of parameters for get_no_artifact_times() function\n",
    "    \"\"\"\n",
    "\n",
    "    def insert_default(self): #This will run if there are no arguments to original class?\n",
    "        #first, insert the default, none, into the table. this contains the parameters that will likely not have any imapct on data. \n",
    "        \"\"\"Insert the default artifact parameters ('none') with a appropriate parameter dict .\n",
    "        \"\"\"\n",
    "        param_dict = {} #empty dictionary called param_dict\n",
    "        param_dict['skip'] = True\n",
    "        param_dict['zscore_thresh'] = -1.0\n",
    "        param_dict['amplitude_thresh'] = -1.0\n",
    "        param_dict['proportion_above_thresh'] = -1.0\n",
    "        param_dict['zero_window_len'] = 30 # 1 ms at 30 KHz, but this is of course skipped\n",
    "        self.insert1({'artifact_parameter_name': 'none', 'parameter_dict' : param_dict},\n",
    "                        skip_duplicates=True)\n",
    "#self is a variable that points to the instance of the method you are working with. \n",
    "\n",
    "#Looks like this is a method that will actually find times with artifact and remove them. \n",
    "#Artifacts are defined as periods where the absolute amplitude of the signal exceeds one or both specified thresholds on the proportion of channels specified, with the period extended by the zero_window/2 samples on each side\n",
    "\n",
    "\n",
    "#SO this section will take the parameters (why are they hard coded here?) \n",
    "#And will return valid times. \n",
    "#DOnt have a primary key? \n",
    "\n",
    "\n",
    "#want this function to be tied to tables info\n",
    "#could go around other dj.manual tables, see what kinds of functions you see in them (do they fetch at the end, etc) \n",
    "#HOw functions are set up in python (regarding the hard coded things)- if you dont insert, it will default to the hard coded things  \n",
    "    #find a way to share notebook for us both to check unestanding \n",
    "    \n",
    "    def get_no_artifact_times(self, recording, zscore_thresh=-1.0, amplitude_thresh=-1.0, \n",
    "                              proportion_above_thresh=1.0, zero_window_len=1.0, skip: bool=True):\n",
    "        #Defines the parameters for artifact detection and their types. \n",
    "        \"\"\"returns an interval list of valid times, excluding detected artifacts found in data within recording extractor.\n",
    "        Artifacts are defined as periods where the absolute amplitude of the signal exceeds one\n",
    "        or both specified thresholds on the proportion of channels specified, with the period extended\n",
    "        by the zero_window/2 samples on each side\n",
    "        Threshold values <0 are ignored.\n",
    "\n",
    "        :param recording: recording extractor\n",
    "        :type recording: SpikeInterface recording extractor object\n",
    "        :param zscore_thresh: Stdev threshold for exclusion, defaults to -1.0\n",
    "        :type zscore_thresh: float, optional\n",
    "        :param amplitude_thresh: Amplitude threshold for exclusion, defaults to -1.0\n",
    "        :type amplitude_thresh: float, optional\n",
    "        :param proportion_above_thresh:\n",
    "        :type float, optional\n",
    "        :param zero_window_len: the width of the window in milliseconds to zero out (window/2 on each side of threshold crossing)\n",
    "        :type int, optional\n",
    "        :return: [array of valid times]\n",
    "        :type: [numpy array]\n",
    "        \"\"\"\n",
    "\n",
    "        # if no thresholds were specified, we return an array with the timestamps of the first and last samples\n",
    "        if zscore_thresh <= 0 and amplitude_thresh <= 0:\n",
    "            return np.asarray([[recording._timestamps[0], recording._timestamps[recording.get_num_frames()-1]]])\n",
    "       \n",
    "    #This is where the artifact detection and comparison to thresholds occurs. \n",
    "    \n",
    "    #use the specified window length (how long to zero out on either sde of the artifact based on sampling rate\n",
    "        \n",
    "        half_window_points = np.round(\n",
    "            recording.get_sampling_frequency() * 1000 * zero_window_len / 2)\n",
    "      \n",
    "    #User defines proportion of electrodes that have to be above threshold for it to be labeled artifact (but two diff versions of this?)\n",
    "    nelect_above = np.round(proportion_above_thresh * data.shape[0])\n",
    "    \n",
    "        # get the data traces\n",
    "        data = recording.get_traces()\n",
    "\n",
    "        # compute the number of electrodes that have to be above threshold based on the number of rows of data\n",
    "        nelect_above = np.round(\n",
    "            proportion_above_thresh * len(recording.get_channel_ids()))\n",
    "\n",
    "        # apply the amplitude threshold (find when the data is above amplitude threshold)\n",
    "        above_a = np.abs(data) > amplitude_thresh\n",
    "\n",
    "        # zscore the data and get the absolute value for thresholding\n",
    "        dataz = np.abs(stats.zscore(data, axis=1))\n",
    "        above_z = dataz > zscore_thresh #find when datas z scare is above specified threshold\n",
    "       \n",
    "    #not sure what ravel does. im guessing that it looks acrross electrodes(?) and finds when the sum of them being over thresh is bigger than neglect_above\n",
    "        above_both = np.ravel(np.argwhere(\n",
    "            np.sum(np.logical_and(above_z, above_a), axis=0) >= nelect_above))\n",
    "        valid_timestamps = recording._timestamps #not sure what this is doing yet.. why are these valid timesteps?\n",
    "        \n",
    "        # for each above threshold point, set the timestamps on either side of it to -1\n",
    "        #So, have a list of valid timesteps, and set times that are \"above both\" plus or minus window points to negative 1...\n",
    "        for a in above_both:\n",
    "            valid_timestamps[a - half_window_points:a +\n",
    "                             half_window_points] = -1\n",
    "\n",
    "        #anything that is not -1 is now a valid timestamp,     \n",
    "        # use get_valid_intervals to find all of the resulting valid times.\n",
    "        #What is get_valid_intervals? Look into these other arguments \n",
    "        #Go find get valid intervals \n",
    "        return get_valid_intervals(valid_timestamps[valid_timestamps != -1], recording.get_sampling_frequency(), 1.5, 0.001)\n",
    "\n",
    "\n",
    "@schema\n",
    "class SpikeSortingRecordingSelection(dj.Manual):\n",
    "    definition = \"\"\"\n",
    "    # Table for holding parameters for each recording that will be spikesorted\n",
    "    -> SortGroup\n",
    "    -> SortInterval\n",
    "    -> SpikeSortingFilterParameters\n",
    "    ---\n",
    "    -> SpikeSortingArtifactDetectionParameters\n",
    "    -> IntervalList\n",
    "    -> LabTeam\n",
    "    \"\"\"\n",
    "#SO, this one is computed.\n",
    "@schema\n",
    "class SpikeSortingRecording(dj.Computed):\n",
    "    definition = \"\"\"\n",
    "    # Schema for storing recording extractors\n",
    "    -> SpikeSortingRecordingSelection\n",
    "    ---\n",
    "    -> IntervalList.proj(sort_interval_list_name='interval_list_name')\n",
    "    recording_extractor_path: varchar(1000)\n",
    "    recording_extractor_object: BLOB # the dictionary that allows kachery to retrieve the extractor\n",
    "    \"\"\"\n",
    "    def make(self, key):\n",
    "        #First, get recording during valid times\n",
    "        sort_interval_valid_times = self.get_sort_interval_valid_times(key) # get valid time intervals\n",
    "        with Timer(label='getting filtered recording extractor', verbose=True): \n",
    "            recording = self.get_filtered_recording_extractor(key) # not sure whats going on here\n",
    "            recording_timestamps = recording._timestamps\n",
    "\n",
    "        # get the artifact detection parameters and apply artifact detection to zero out artifacts (implementation?) \n",
    "        artifact_key = (SpikeSortingRecordingSelection & key).fetch1( #key will be the name.. wilbur, default)\n",
    "            'artifact_parameter_name')\n",
    "        artifact_param_dict = (SpikeSortingArtifactDetectionParameters & { #get dictionary of parameters)\n",
    "                               'artifact_parameter_name': artifact_key}).fetch1('parameter_dict')\n",
    "        #check to see if we want to do this at all\n",
    "        if not artifact_param_dict['skip']:\n",
    "            no_artifact_valid_times = SpikeSortingArtifactDetectionParameters().get_no_artifact_times(\n",
    "                recording, **artifact_param_dict) #** =unpacking\n",
    "            # update the sort interval valid times to exclude the artifacts\n",
    "            sort_interval_valid_times = interval_list_intersect(\n",
    "                sort_interval_valid_times, no_artifact_valid_times)\n",
    "            # exclude the invalid times\n",
    "            mask = np.full(recording.get_num_frames(), True, dtype='bool')\n",
    "            excluded_ind = interval_list_excludes_ind(\n",
    "                sort_interval_valid_times, recording_timestamps)\n",
    "            if len(excluded_ind) > 0:\n",
    "                mask[interval_list_excludes_ind(\n",
    "                    sort_interval_valid_times, recording_timestamps)] = False\n",
    "            recording = st.preprocessing.mask(recording, mask)\n",
    "\n",
    "        #add the sort_interval_valid_times as an interval list\n",
    "        tmp_key = {}\n",
    "        tmp_key['nwb_file_name'] = key['nwb_file_name']\n",
    "        tmp_key['interval_list_name'] = key['nwb_file_name'] + '_' + \\\n",
    "            key['sort_interval_name'] + '_' + str(key['sort_group_id'])+ \\\n",
    "            key['filter_parameter_set_name'] + '_recording'\n",
    "        tmp_key['valid_times'] = sort_interval_valid_times;\n",
    "        IntervalList.insert1(tmp_key, replace=True)\n",
    "\n",
    "        # store the list of valid times for the sort\n",
    "        key['sort_interval_list_name'] = tmp_key['interval_list_name']\n",
    "\n",
    "        # Path to files that will hold the recording extractors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "439a1fa6-e0c5-417d-b151-974386261f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions: Mostly about implementation, the concepts make sense to me. \n",
    "# How to “call” methods in the class with specific arguments? \n",
    "# The arrows? Why some primary keys (?) have arrows before? \n",
    "# Whats def make again? \n",
    "# What is a recording extractor? \n",
    "# What is actually returned from the original parameters class? It looks like its valid times but its also just parameters… \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e14f75-742e-4818-8b21-ae93a3a194a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to call these things, see whats going on.... a few ways. \n",
    "#Take these out of schema setups, but keep orgaization. Keep the functions, etc. dismantle from datajointiness, make familiar. Slack when i have questions \n",
    "#getting a recording extractor by using spike sorting code (could use beans data- use a tiny amount of data!! super fast!) 5 seconds! beans will be smaller\n",
    "#how does current spikesorting work? knowing the order of tables that gets filled in (not detail I went rhough today) - relational diagrams. \n",
    "#Is there an updated spikesorting notebook? Might be on slack. Basic high level flow, and understanding where recording extractor is(what is it, data type). How would I use that in these functions?  \n",
    "#when i get to that point, send a slack mesage "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nwb_datajoint] *",
   "language": "python",
   "name": "conda-env-nwb_datajoint-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
